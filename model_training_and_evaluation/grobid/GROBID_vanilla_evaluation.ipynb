{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages & Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "import pandas as pd\n",
    "import os\n",
    "# PdfMiner\n",
    "import glob\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "# Token Vectorization\n",
    "from langdetect import detect \n",
    "import fasttext.util\n",
    "import fasttext\n",
    "########################################\n",
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "########################################\n",
    "import regex as re  \n",
    "import string\n",
    "import re\n",
    "########################################\n",
    "from datetime import datetime\n",
    "import collections\n",
    "########################################\n",
    "# Keras imports for ML Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tei(tei_file):\n",
    "    with open(tei_file, 'r',encoding=\"utf-8\") as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "        return soup\n",
    "    raise RuntimeError('Cannot generate a soup from the input')\n",
    "    \n",
    "def elem_to_text(elem, default=''):\n",
    "    if elem:\n",
    "        return elem.getText()\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "@dataclass\n",
    "class Person:\n",
    "    firstname: str\n",
    "    middlename: str\n",
    "    surname: str\n",
    "    \n",
    "class TEIFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.soup = read_tei(filename)\n",
    "        self._text = None\n",
    "        self._title = ''\n",
    "        self._abstract = ''\n",
    "\n",
    "    @property\n",
    "    def doi(self):\n",
    "        idno_elem = self.soup.find('idno', type='DOI')\n",
    "        if not idno_elem:\n",
    "            return ''\n",
    "        else:\n",
    "            return idno_elem.getText()\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        if not self._title:\n",
    "            self._title = self.soup.title.getText()\n",
    "        return self._title\n",
    "\n",
    "    @property\n",
    "    def abstract(self):\n",
    "        if not self._abstract:\n",
    "            abstract = self.soup.abstract.getText(separator=' ', strip=True)\n",
    "            self._abstract = abstract\n",
    "        return self._abstract\n",
    "\n",
    "    @property\n",
    "    def authors(self):\n",
    "        authors_in_header = self.soup.analytic.find_all('author')\n",
    "\n",
    "        result = []\n",
    "        for author in authors_in_header:\n",
    "            persname = author.persname\n",
    "            if not persname:\n",
    "                continue\n",
    "            firstname = elem_to_text(persname.find(\"forename\", type=\"first\"))\n",
    "            middlename = elem_to_text(persname.find(\"forename\", type=\"middle\"))\n",
    "            surname = elem_to_text(persname.surname)\n",
    "            person = Person(firstname, middlename, surname)\n",
    "            result.append(person)\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        if not self._text:\n",
    "            divs_text = []\n",
    "            for div in self.soup.body.find_all(\"div\"):\n",
    "                # div is neither an appendix nor references, just plain text.\n",
    "                if not div.get(\"type\"):\n",
    "                    div_text = div.get_text(separator=' ', strip=True)\n",
    "                    divs_text.append(div_text)\n",
    "\n",
    "            plain_text = \" \".join(divs_text)\n",
    "            self._text = plain_text\n",
    "        return self._text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that get the fullpath of files in a directory\n",
    "def listdir_fullpath(d):\n",
    "    return [os.path.join(d, f) for f in os.listdir(d)]\n",
    "\n",
    "### Function that remove new lines\n",
    "def removePassage(my_str):\n",
    "    my_str1 = re.sub(\"\\\\\\\\ud\", \" \", my_str)\n",
    "    my_str2 = re.sub(\"\\\\\\\\n\", \" \", my_str1)\n",
    "    return(my_str2)\n",
    "\n",
    "### Function that parse the first page of a PDF\n",
    "def extract_page_one(path):\n",
    "    output_string = StringIO()\n",
    "    \n",
    "    with open(path, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        interpreter.process_page(list(PDFPage.create_pages(doc))[0])\n",
    "        return(output_string)\n",
    "    \n",
    "\n",
    "\n",
    "### \"Chronological left join\" for adding labelled new line tokens (\"NEWLINE\")\n",
    "def add_newlines(Tokens,Real_Tokens,y_final):\n",
    "    y_final_REAL = []\n",
    "    k = 0\n",
    "    m = 0\n",
    "    for i in range(len(Tokens)):\n",
    "        if k == 0:\n",
    "            j=i\n",
    "        else:\n",
    "            if m == 0:\n",
    "                j = k+1\n",
    "            else:\n",
    "                j = m+1\n",
    "        if Tokens[i] == Real_Tokens[j] : # If tokens are the same, then take y_final_REAL\n",
    "            y_final_REAL.append(y_final[i])\n",
    "            m = j\n",
    "        else:\n",
    "            for k in range(j,len(Real_Tokens)): # Else go through Real_Tokens until there is a match\n",
    "\n",
    "                if Real_Tokens[k] == 'NEWLINE':\n",
    "                    y_final_REAL.append('miscellaneous')\n",
    "\n",
    "                else:\n",
    "                    y_final_REAL.append(y_final[i])\n",
    "                    m=k\n",
    "                    break\n",
    "\n",
    "    RealTokens_final = Real_Tokens[:len(y_final_REAL)]\n",
    "    ## It is possible that several NEWLINES come after each other. Therefore we use \"end_title>i\" to determine, if there is a title label afterwards in the vector.        \n",
    "    index_title = [i for i, e in enumerate(y_final_REAL) if e == 'I-title']\n",
    "    if index_title==[]:\n",
    "        return(RealTokens_final,y_final_REAL)\n",
    "    else:\n",
    "        end_title = max(index_title)\n",
    "\n",
    "        ### lable NEWLINES in title as \"I-title\"        \n",
    "        for i in range(len(RealTokens_final)):\n",
    "            if RealTokens_final[i]=='NEWLINE':\n",
    "                if (y_final_REAL[i+1] =='I-title' or end_title>i) and y_final_REAL[i-1] in ('B-title','I-title'):\n",
    "                    y_final_REAL[i] = 'I-title'\n",
    "        ## It is possible that several NEWLINES come after each other. Therefore we use \"end_title>i\" to determine, if there is a title label afterwards in the vector.        \n",
    "\n",
    "        return(RealTokens_final,y_final_REAL)\n",
    "\n",
    "\n",
    "### Grobid: remove [[...]]\n",
    "### Function that remove \"arrays\" from the author fields\n",
    "def removeAutor_grobid(my_str):\n",
    "    my_str1 = re.sub(\"\\[\\['\", \"\", my_str) \n",
    "    my_str2 = re.sub(\"'\\]\\]\", \"\", my_str1)\n",
    "    my_str3 = re.sub(\"'\", \"\", my_str2) \n",
    "    return(my_str3)\n",
    "\n",
    "### Function that remove \"arrays\" from the author fields\n",
    "def removeAutor(my_str):\n",
    "    my_str1 = re.sub(\"\\['\", \"\", my_str)\n",
    "    my_str2 = re.sub(\"'\\]\", \"\", my_str1)\n",
    "    my_str3 = re.sub(\"'\", \"\", my_str2)\n",
    "    return(my_str3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read TEI Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT RUN! Predictions of GROBID are already saved in 'grobid_16467.csv'\n",
    "### Transform the tei.xml files into a dataframe\n",
    "\n",
    "\n",
    "all_files = listdir_fullpath(\"/_final_selection_16478/GROBID_ALL\")\n",
    "\n",
    "\n",
    "Frame = pd.DataFrame({\"core_id\": [] , \"title\": [] , \"authors\": [] })\n",
    "i = 0\n",
    "for tei_doc in all_files:\n",
    "    tei = TEIFile(tei_doc)\n",
    "    core_id = re.sub(\".tei.xml\",\"\",re.sub(\"D:\\\\\\\\_final_selection_16478\\\\\\\\GROBID_ALL\\\\\\\\Core_ID_\",\"\",tei_doc))\n",
    "    authors = []\n",
    "    for i in range(len(tei.authors)):\n",
    "        if len(tei.authors[i].firstname)==1:\n",
    "            forename = tei.authors[i].firstname + \". \" + tei.authors[i].middlename + \".\"\n",
    "            surname = tei.authors[i].surname\n",
    "            name = [forename , surname]\n",
    "        elif len(tei.authors[i].middlename)==1:\n",
    "            forename = tei.authors[i].firstname +\" \" +tei.authors[i].middlename + \".\"\n",
    "            surname = tei.authors[i].surname\n",
    "            name = [forename , surname]\n",
    "        else:\n",
    "            forename = tei.authors[i].firstname + \" \" +tei.authors[i].middlename\n",
    "            surname = tei.authors[i].surname\n",
    "            name = [forename , surname]\n",
    "        authors.append(name)\n",
    "    \n",
    "    \n",
    "    Frame = Frame.append(pd.DataFrame(data = {\"core_id\": core_id , \"title\":tei.title , \"authors\":str(authors)},index = [i]), ignore_index=True)\n",
    "    i =+ 1\n",
    "\n",
    "Frame.to_excel(\"grobid_16467.xlsx\")\n",
    "Frame.to_csv('grobid_16467.csv')\n",
    "Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate GROBID prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_grobid_pred = pd.read_csv('grobid_16467.csv', sep = ',')#  , encoding= 'utf-16')\n",
    "df_grobid_pred.drop('Unnamed: 0' , axis = 1 , inplace=True)\n",
    "df_grobid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_9620 = \"D:/PDF_Grobid_train_eval/\"\n",
    "\n",
    "pdf = os.listdir(path_9620)\n",
    "\n",
    "\n",
    "train_files_core_id = []\n",
    "train_files_paths = []\n",
    "for elem in pdf:\n",
    "    core = int(elem.replace(\".pdf\",\"\").replace(\"Core_ID_\",\"\"))\n",
    "    if core in list(df_grobid_pred.core_id):\n",
    "        train_files_core_id.append(core)\n",
    "#         files_paths.append(\"D:/_final_selection_16478/all_pdf/\" + elem)\n",
    "        train_files_paths.append(path_9620 + \"Core_ID_\"+str(core)+\".pdf\")\n",
    "print(len(train_files_core_id))\n",
    "print(train_files_core_id[0])\n",
    "print(train_files_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grobid_pred = df_grobid_pred[df_grobid_pred.core_id.isin(train_files_core_id)].reset_index()\n",
    "df_grobid_pred.drop('index' , axis = 1 , inplace=True)\n",
    "print(df_grobid_pred.shape)\n",
    "print(\"{} titles and {} authors are NA\".format(sum(df_grobid_pred.title.isna()),sum(df_grobid_pred.authors == \"[]\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### extract text \n",
    "\n",
    "all_pdf_text = [] \n",
    "start_time = datetime.now()\n",
    "for i in range(len(train_files_paths)):\n",
    "    try:\n",
    "        all_pdf_text.append(extract_page_one(train_files_paths[i]).getvalue())\n",
    "        if i % 500 == 0:\n",
    "            print(str((i/len(train_files_paths))*100)+'%')\n",
    "    except:\n",
    "        all_pdf_text.append(\"Not readable\")\n",
    "        print(\"ERROR\")\n",
    "    \n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import REAL Meta Data\n",
    "\n",
    "df_meta_real = pd.read_csv('metadata_15553.csv', sep = ',')#  , encoding= 'utf-16')\n",
    "# df_meta.drop('Unnamed: 0' , axis = 1 , inplace=True)\n",
    "# df_meta_real\n",
    "\n",
    "### get all titles from meta data with core_ids of fulltext\n",
    "titles_real = []\n",
    "for i in range(len(train_files_core_id)):\n",
    "    index = df_meta_real.index[df_meta_real['coreId'] == int(train_files_core_id[i])].tolist()\n",
    "    if index == []:\n",
    "        titles_real.append('metadata not found')\n",
    "    else: \n",
    "        index = index[0]\n",
    "        title_pdf  = df_meta_real.loc[index,'title']\n",
    "        titles_real.append(title_pdf)\n",
    "len(titles_real)\n",
    "\n",
    "### Get authors for the PDF´s\n",
    "authors_real = []\n",
    "for i in range(len(train_files_core_id)):\n",
    "    index = df_meta_real.index[df_meta_real['coreId'] == int(train_files_core_id[i])].tolist()\n",
    "    index = index[0]\n",
    "    author_pdf  = df_meta_real.loc[index,'authors']\n",
    "\n",
    "    author_pdf = removeAutor(author_pdf).split(\",\")\n",
    "    for j in range(len(author_pdf)):\n",
    "        author_pdf[j] = ' '.join(author_pdf[j].split()) ## remove excessive whitespaces (auch am Anfang)\n",
    "        \n",
    "    authors_real.append(author_pdf)\n",
    "len(authors_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Label vectors for the GROBID Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all titles from grobid predictions with core_ids of fulltext\n",
    "titles = []\n",
    "for i in range(len(df_grobid_pred.core_id)):\n",
    "    index = df_grobid_pred.index[df_grobid_pred['core_id'] == int(df_grobid_pred.core_id[i])].tolist()\n",
    "    if df_grobid_pred.title.isna()[i]:\n",
    "        titles.append('title was not predicted')\n",
    "    else: \n",
    "        index = index[0]\n",
    "        title_pdf  = df_grobid_pred.loc[index,'title']\n",
    "        titles.append(title_pdf)\n",
    "\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get author for the PDF´s\n",
    "\n",
    "authors = []\n",
    "for i in range(len(df_grobid_pred.core_id)):\n",
    "    index = df_grobid_pred.index[df_grobid_pred['core_id'] == int(df_grobid_pred.core_id[i])].tolist()\n",
    "    index = index[0]\n",
    "    author_pdf  = df_grobid_pred.loc[index,'authors']\n",
    "\n",
    "    author_pdf = removeAutor_grobid(author_pdf).split(\",\")\n",
    "    for j in range(len(author_pdf)):\n",
    "        author_pdf[j] = ' '.join(author_pdf[j].split()) ## remove excessive whitespace\n",
    "        \n",
    "    authors.append(author_pdf)\n",
    "len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### function for data labelling\n",
    "\n",
    "no_author = []\n",
    "no_title = []\n",
    "error_papers = []\n",
    "result_tokens = []\n",
    "result_label = []\n",
    "### Real Meta\n",
    "result_label_real = []\n",
    "###\n",
    "\n",
    "count_papers = len(df_grobid_pred.core_id)\n",
    "\n",
    "for paper in range(count_papers):\n",
    "\n",
    "    title = ' '.join(removePassage(titles[paper]).split()).lower() # Remove excces Whitespace & to lowercase\n",
    "    title = re.sub(\"\\(\",\"\\(\",title) # () as non-regex string\n",
    "    title = re.sub(\"\\)\",\"\\)\",title)\n",
    "    title = re.sub(\"\\*\",\"\\*\",title) # * as non-regex string\n",
    "\n",
    "    title_index = re.search(title, ' '.join(all_pdf_text[paper].split()).lower()) # search for the title\n",
    "    \n",
    "    #### Real Meta\n",
    "    title_real = ' '.join(removePassage(titles_real[paper]).split()).lower() # Remove excces Whitespace & to lowercase\n",
    "    title_real = re.sub(\"\\(\",\"\\(\",title_real) # () as non-regex string\n",
    "    title_real = re.sub(\"\\)\",\"\\)\",title_real)\n",
    "    title_real = re.sub(\"\\*\",\"\\*\",title_real) # * as non-regex string\n",
    "\n",
    "    title_index_real = re.search(title_real, ' '.join(all_pdf_text[paper].split()).lower()) # search for the title\n",
    "    ######\n",
    "    \n",
    "    print('CoreID:  ' + str(train_files_core_id[paper]))\n",
    "\n",
    "    if title_index==None:\n",
    "        Text_pdf_0 = ' '.join(all_pdf_text[paper].split())\n",
    "        \n",
    "        no_title.append(df_grobid_pred.core_id[paper])\n",
    "        y_final= np.repeat('miscellaneous',len(Text_pdf_0.split()))\n",
    "    else:\n",
    "        \n",
    "        Text_pdf_0 = ' '.join(all_pdf_text[paper].split())\n",
    "\n",
    "        ##### TITLE ################################################\n",
    "        if title_index.start()==0:\n",
    "            teil_B = \"\"\n",
    "        else:\n",
    "            teil_B = Text_pdf_0[0:title_index.start()-1]\n",
    "        teil_T = Text_pdf_0[title_index.start():title_index.end()]\n",
    "        teil_E = Text_pdf_0[title_index.end()+1:len(Text_pdf_0)]\n",
    "\n",
    "        y_teil1 = np.repeat('miscellaneous',len(teil_B.split()))\n",
    "        y_teil2 = np.append(['B-title'],np.repeat('I-title',len(teil_T.split())-1))\n",
    "        y_teil3 = np.repeat('miscellaneous',len(teil_E.split()))\n",
    "\n",
    "        y_final = np.concatenate((y_teil1, y_teil2 , y_teil3), axis=None)\n",
    "        \n",
    "    ### Real Meta    \n",
    "    if title_index_real!=None:\n",
    "        Text_pdf_0 = ' '.join(all_pdf_text[paper].split())\n",
    "\n",
    "        ##### TITLE ################################################\n",
    "        if title_index_real.start()==0:\n",
    "            teil_B_real = \"\"\n",
    "        else:\n",
    "            teil_B_real = Text_pdf_0[0:title_index_real.start()-1]\n",
    "        teil_T_real = Text_pdf_0[title_index_real.start():title_index_real.end()]\n",
    "        teil_E_real = Text_pdf_0[title_index_real.end()+1:len(Text_pdf_0)]\n",
    "\n",
    "        y_teil1_r = np.repeat('miscellaneous',len(teil_B_real.split()))\n",
    "        y_teil2_r = np.append(['B-title'],np.repeat('I-title',len(teil_T_real.split())-1))\n",
    "        y_teil3_r = np.repeat('miscellaneous',len(teil_E_real.split()))\n",
    "\n",
    "        y_final_real = np.concatenate((y_teil1_r, y_teil2_r , y_teil3_r), axis=None)\n",
    "    ###\n",
    "    \n",
    "    ##### Get Text\n",
    "    all_pdf_text1 = re.sub(\"\\\\n\",\" NEWLINE \",all_pdf_text[paper])\n",
    "    Text_pdf_0_NL = ' '.join(all_pdf_text1.split())\n",
    "\n",
    "    Tokens = Text_pdf_0.split()\n",
    "    Labels = y_final\n",
    "    Real_Tokens = Text_pdf_0_NL.split()\n",
    "\n",
    "    Tokens = all_pdf_text[paper].split()\n",
    "\n",
    "    Tokens_final_lower = []\n",
    "    for i in range(len(Tokens)):\n",
    "        Tokens_final_lower.append(Tokens[i].lower())\n",
    "    try:\n",
    "        if authors[paper]!= ['[]']:\n",
    "\n",
    "            authors_surname = []\n",
    "            for i in range(len(authors[paper])):\n",
    "                if i % 2 == 0:\n",
    "                    authors_surname.append(authors[paper][i])\n",
    "\n",
    "            authors_surname_lower = []\n",
    "            for i in range(len(authors_surname)):\n",
    "                authors_surname_lower.append(authors_surname[i].lower())\n",
    "\n",
    "            if re.match('.\\.',authors[paper][1]) == None:\n",
    "                authors_forename = []\n",
    "                for i in range(len(authors[paper])):\n",
    "                    if i % 2 == 1:\n",
    "                        authors_forename.append(authors[paper][i].split())\n",
    "\n",
    "                authors_forename = list(np.concatenate((authors_forename), axis=None))\n",
    "                authors_forename_lower = []\n",
    "                for i in range(len(authors_forename)):\n",
    "                    authors_forename_lower.append(authors_forename[i].lower())\n",
    "\n",
    "                authors_surname_lower = list(np.concatenate((authors_forename_lower,authors_surname_lower), axis=None))\n",
    "\n",
    "\n",
    "            vec_author = []\n",
    "            for token in Tokens_final_lower:\n",
    "                line = any(word in token for word in authors_surname_lower)\n",
    "                vec_author.append(line)\n",
    "\n",
    "            index_author = [i for i, e in enumerate(vec_author) if e == True]\n",
    "\n",
    "            if title_index!=None:\n",
    "                if len(index_author)>(len(authors_surname_lower)):\n",
    "                    diff = len(index_author) - len(authors_surname_lower)\n",
    "                    dist = []\n",
    "                    for j in range(len(index_author)):\n",
    "                        dist.append(abs(index_author[j]-np.where(y_final==\"B-title\")[0][0]))\n",
    "\n",
    "                    dict1 = dict(zip(dist , index_author))\n",
    "\n",
    "                    dist.sort(reverse = True)\n",
    "\n",
    "                    for k in range(len(dist[0:diff])):\n",
    "                        vec_author[dict1[dist[0:diff][k]]] = False\n",
    "\n",
    "            for i in range(len(y_final)):\n",
    "                if vec_author[i] == True:\n",
    "                    y_final[i] = 'author'\n",
    "\n",
    "            if True not in vec_author:\n",
    "                no_author.append(train_files_core_id[paper])\n",
    "\n",
    "            if re.match('.\\.',authors[paper][1]) != None:\n",
    "\n",
    "                index_author_true = [i for i, e in enumerate(vec_author) if e == True]\n",
    "\n",
    "                for w in range(len(index_author_true)):\n",
    "                    index = index_author_true[w]\n",
    "                    for t in range(index - 4,index + 4):\n",
    "                        if re.match('.\\.',Tokens_final_lower[t]) != None and Tokens[t].isupper():\n",
    "                            y_final[t] = 'author'\n",
    "        ### Real Meta\n",
    "        authors_surname_real = []\n",
    "        for i in range(len(authors_real[paper])):\n",
    "            if i % 2 == 0:\n",
    "                authors_surname_real.append(authors_real[paper][i])\n",
    "\n",
    "        authors_surname_lower_real = []\n",
    "        for i in range(len(authors_surname_real)):\n",
    "            authors_surname_lower_real.append(authors_surname_real[i].lower())\n",
    "\n",
    "        if re.match('.\\.',authors_real[paper][1]) == None:\n",
    "            authors_forename_real = []\n",
    "            for i in range(len(authors_real[paper])):\n",
    "                if i % 2 == 1:\n",
    "                    authors_forename_real.append(authors_real[paper][i].split())\n",
    "\n",
    "            authors_forename_real = list(np.concatenate((authors_forename_real), axis=None))\n",
    "            authors_forename_lower_real = []\n",
    "            for i in range(len(authors_forename_real)):\n",
    "                authors_forename_lower_real.append(authors_forename_real[i].lower())\n",
    "\n",
    "            authors_surname_lower_real = list(np.concatenate((authors_forename_lower_real,authors_surname_lower_real), axis=None))\n",
    "\n",
    "\n",
    "        vec_author_real = []\n",
    "        for token in Tokens_final_lower:\n",
    "            line_real = any(word in token for word in authors_surname_lower_real)\n",
    "            vec_author_real.append(line_real)\n",
    "\n",
    "        index_author_real = [i for i, e in enumerate(vec_author_real) if e == True]\n",
    "\n",
    "        if title_index_real!=None:\n",
    "            if len(index_author_real)>(len(authors_surname_lower_real)):\n",
    "                diff = len(index_author_real) - len(authors_surname_lower_real)\n",
    "                dist = []\n",
    "                for j in range(len(index_author_real)):\n",
    "                    dist.append(abs(index_author_real[j]-np.where(y_final_real==\"B-title\")[0][0]))\n",
    "\n",
    "                dict1 = dict(zip(dist , index_author_real))\n",
    "\n",
    "                dist.sort(reverse = True)\n",
    "\n",
    "                for k in range(len(dist[0:diff])):\n",
    "                    vec_author_real[dict1[dist[0:diff][k]]] = False\n",
    "\n",
    "        for i in range(len(y_final_real)):\n",
    "            if vec_author_real[i] == True:\n",
    "                y_final_real[i] = 'author'\n",
    "\n",
    "\n",
    "        if re.match('.\\.',authors_real[paper][1]) != None:\n",
    "\n",
    "            index_author_true_real = [i for i, e in enumerate(vec_author_real) if e == True]\n",
    "\n",
    "            for w in range(len(index_author_true_real)):\n",
    "                index = index_author_true_real[w]\n",
    "                for t in range(index - 4,index + 4):\n",
    "                    if re.match('.\\.',Tokens_final_lower[t]) != None and Tokens[t].isupper():\n",
    "                        y_final_real[t] = 'author'\n",
    "        ###\n",
    "\n",
    "        RealTokens_final = add_newlines(Tokens,Real_Tokens,y_final)[0]\n",
    "        y_final_REAL = add_newlines(Tokens,Real_Tokens,y_final)[1]\n",
    "        ### Real Meta\n",
    "        y_final_REAL2 = add_newlines(Tokens,Real_Tokens,y_final_real)[1]\n",
    "        result_label_real.append(y_final_REAL2)\n",
    "        ###\n",
    "        result_label.append(y_final_REAL)\n",
    "        result_tokens.append(RealTokens_final)\n",
    "    except:\n",
    "        error_papers.append(train_files_core_id[paper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(no_author))\n",
    "print(len(no_title))\n",
    "print(len(error_papers))\n",
    "\n",
    "print(len(result_label))\n",
    "print(len(result_tokens))\n",
    "print(len(result_label_real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of GROBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_precision = []\n",
    "all_recall = []\n",
    "for i in range(len(result_tokens)):\n",
    "\n",
    "    y_true = [int(t.replace(\"B-title\",'0').replace(\"I-title\",'1').replace(\"author\",'2').replace(\"miscellaneous\",'3')) for t in result_label_real[i]]\n",
    "    y_pred = [int(t.replace(\"B-title\",'0').replace(\"I-title\",'1').replace(\"author\",'2').replace(\"miscellaneous\",'3')) for t in result_label[i]]\n",
    "    # target_names = ['class 0', 'class 1', 'class 2']\n",
    "    #Get the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    #array([[1, 0, 0],\n",
    "    #   [1, 0, 0],\n",
    "    #   [0, 1, 2]])\n",
    "    true_pos = cm.diagonal()\n",
    "\n",
    "    false_pos = np.sum(cm, axis=0) - true_pos\n",
    "    false_neg = np.sum(cm, axis=1) - true_pos\n",
    "\n",
    "\n",
    "    precision = (true_pos / (true_pos + false_pos))\n",
    "    \n",
    "    precision2 = []\n",
    "    for p in precision:\n",
    "        if math.isnan(p):\n",
    "            precision2.append(0)\n",
    "        else:\n",
    "            precision2.append(p)\n",
    "    if len(precision2)!=4:\n",
    "        precision2.append(0)\n",
    "    recall = (true_pos / (true_pos + false_neg))\n",
    "    \n",
    "    recall2 = []\n",
    "    for r in recall:\n",
    "        if math.isnan(r):\n",
    "            recall2.append(0)\n",
    "        else:\n",
    "            recall2.append(r)\n",
    "    if len(recall2)!=4:\n",
    "        recall2.append(0)\n",
    "    all_precision.append(np.array(precision2))\n",
    "    all_recall.append(np.array(recall2))\n",
    "    \n",
    "pm = np.array(all_precision)\n",
    "rm = np.array(all_recall)\n",
    "\n",
    "precision = pm.sum(axis=0)/len(pm)\n",
    "recall = rm.sum(axis=0)/len(rm)\n",
    "f1_score = 2*(precision*recall)/(precision+recall)\n",
    "Classes = [\"B-title\",\"I-title\",\"author\",\"miscellaneous\"]\n",
    "for c,p,r,f in zip(Classes,precision,recall,f1_score):\n",
    "    print(\"Scores for label {}: \\nPrecision ==> {} \\nRecall ==> {} \\nF1-Score ==> {}\\n\".format(c,p,r,f))\n",
    "    \n",
    "overall_precison = np.mean(precision[:3])\n",
    "overall_recall = np.mean(recall[:3])\n",
    "overall_f1_score = np.mean(f1_score[:3])\n",
    "\n",
    "print(\"Scores over all classes (except \\\"miscellaneous\\\"):\\nPrecision ==> {} \\nRecall ==> {} \\nF1-Score ==> {}\\n\".format(overall_precison,overall_recall,overall_f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
