{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Data Matching\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "import pandas as pd\n",
    "import os\n",
    "# PdfMiner\n",
    "import glob\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "########################################\n",
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "########################################\n",
    "import regex as re  \n",
    "import string\n",
    "import re\n",
    "########################################\n",
    "from datetime import datetime\n",
    "import collections\n",
    "########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function that remove new lines\n",
    "def removePassage(my_str):\n",
    "    my_str1 = re.sub(\"\\\\\\\\ud\", \" \", my_str)\n",
    "    my_str2 = re.sub(\"\\\\\\\\n\", \" \", my_str1)\n",
    "    return(my_str2)\n",
    "\n",
    "### Function that parse the first page of a PDF\n",
    "def extract_page_one(path):\n",
    "    output_string = StringIO()\n",
    "    \n",
    "    with open(path, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        interpreter.process_page(list(PDFPage.create_pages(doc))[0])\n",
    "        return(output_string)\n",
    "    \n",
    "### Function that remove \"arrays\" from the author fields\n",
    "def removeAutor(my_str):\n",
    "    my_str1 = re.sub(\"\\['\", \"\", my_str)\n",
    "    my_str2 = re.sub(\"'\\]\", \"\", my_str1)\n",
    "    my_str3 = re.sub(\"'\", \"\", my_str2)\n",
    "    return(my_str3)\n",
    "\n",
    "### \"Chronological left join\" for adding labelled new line tokens (\"NEWLINE\")\n",
    "def add_newlines(Tokens,Real_Tokens,y_final):\n",
    "    y_final_REAL = []\n",
    "    k = 0\n",
    "    m = 0\n",
    "    for i in range(len(Tokens)):\n",
    "        if k == 0:\n",
    "            j=i\n",
    "        else:\n",
    "            if m == 0:\n",
    "                j = k+1\n",
    "            else:\n",
    "                j = m+1\n",
    "        if Tokens[i] == Real_Tokens[j] : # If tokens are the same, then take y_final_REAL\n",
    "            y_final_REAL.append(y_final[i])\n",
    "            m = j\n",
    "        else:\n",
    "            for k in range(j,len(Real_Tokens)): # Else go through Real_Tokens until there is a match\n",
    "\n",
    "                if Real_Tokens[k] == 'NEWLINE':\n",
    "                    y_final_REAL.append('miscellaneous')\n",
    "                \n",
    "\n",
    "                else:\n",
    "                    y_final_REAL.append(y_final[i])\n",
    "                    m=k\n",
    "                    break\n",
    "\n",
    "    RealTokens_final = Real_Tokens[:len(y_final_REAL)]\n",
    "    \n",
    "    index_title = [i for i, e in enumerate(y_final_REAL) if e == 'I-title']\n",
    "    end_title = max(index_title)\n",
    "    \n",
    "    ### lable NEWLINES in title as \"I-title\"          \n",
    "    for i in range(len(RealTokens_final)):\n",
    "        if RealTokens_final[i]=='NEWLINE':\n",
    "            if (y_final_REAL[i+1] =='I-title' or end_title>i) and y_final_REAL[i-1] in ('B-title','I-title'):\n",
    "                y_final_REAL[i] = 'I-title'\n",
    "    ## It is possible that several NEWLINES come after each other. Therefore we use \"end_title>i\" to determine, if there is a title label afterwards in the vector.                    \n",
    "    return(RealTokens_final,y_final_REAL)\n",
    "\n",
    "#Create Word Vector representation\n",
    "def detect_and_vectorize(tokens_sequence): #### input is the tokens list of ONE PAPER e.g. result_tokens[2]\n",
    "    \n",
    "    tokens_vectorized = []\n",
    "    lang = detect(' '.join(tokens_sequence))\n",
    "    \n",
    "    if (lang == 'ru'):\n",
    "        for i in range(len(tokens_sequence)):\n",
    "            tokens_vectorized.append(np.float16(ft_ru.get_word_vector(tokens_sequence[i])))\n",
    "                \n",
    "    elif (lang == 'bg'):\n",
    "        for i in range(len(tokens_sequence)):\n",
    "            tokens_vectorized.append(np.float16(ft_bg.get_word_vector(tokens_sequence[i])))\n",
    "                \n",
    "    else:  ## assume language == uk\n",
    "        for i in range(len(tokens_sequence)):\n",
    "            tokens_vectorized.append(np.float16(ft_uk.get_word_vector(tokens_sequence[i])))\n",
    "    \n",
    "    while len(tokens_vectorized)<1000:\n",
    "        tokens_vectorized.append(np.zeros(60))\n",
    "      \n",
    "    if len(tokens_vectorized)>1000:\n",
    "        del tokens_vectorized[1000:] \n",
    "\n",
    "    return np.array(tokens_vectorized)\n",
    "\n",
    "### Additional Features\n",
    "punctuations = '''!()[]{};:'\"\\<>/?@#$%^&*«»_–~.,-'''\n",
    "\n",
    "def compute_additional_features(tokens_sequence): #### input is the tokens list of ONE PAPER e.g. result_tokens[2]\n",
    "    \n",
    "    tokens = tokens_sequence\n",
    "    feature_upper = []\n",
    "    feature_capitalized = []\n",
    "    feature_author_format = []\n",
    "    feature_punctation = []\n",
    "    feature_newline = []\n",
    "    feature_array = []\n",
    "    \n",
    "    while len(tokens)<1000:\n",
    "        tokens.append(str(0))\n",
    "    if len(tokens)>1000:\n",
    "        del tokens[1000:] \n",
    "    #print(tokens)    \n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if str(tokens[i]).isupper():\n",
    "                feature_upper.append(1)\n",
    "\n",
    "            else:\n",
    "                feature_upper.append(0)\n",
    "        else: \n",
    "            feature_upper.append(0)\n",
    "\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if str(tokens[i][0]).isupper():\n",
    "                feature_capitalized.append(1)\n",
    "\n",
    "            else:\n",
    "                feature_capitalized.append(0)\n",
    "        else: \n",
    "            feature_capitalized.append(0)\n",
    "\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if re.match('.\\.',str(tokens[i])) != None and str(tokens[i]).isupper():\n",
    "                feature_author_format.append(1)\n",
    "\n",
    "            else:\n",
    "                feature_author_format.append(0)\n",
    "        else: \n",
    "            feature_author_format.append(0)\n",
    "\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if any((c in punctuations) for c in str(tokens[i])):\n",
    "                feature_punctation.append(1)\n",
    "            else:\n",
    "                feature_punctation.append(0)\n",
    "        else: \n",
    "            feature_punctation.append(0)\n",
    "                \n",
    "        if tokens[i] =='NEWLINE':\n",
    "            feature_newline.append(1)\n",
    "        else: \n",
    "            feature_newline.append(0)\n",
    "    df = pd.DataFrame(list(zip(feature_upper, feature_capitalized,feature_author_format ,feature_punctation,feature_newline)))  \n",
    "    feature_array = df.to_numpy(copy=True)\n",
    "    \n",
    "    return np.array(feature_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Meta Data\n",
    "path_meta=\"D:/final_items_15553.csv\"\n",
    "df_meta = pd.read_csv(path_meta, sep = ',')#  , encoding= 'utf-16')\n",
    "# df_meta.drop('Unnamed: 0' , axis = 1 , inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"D:/PDFs_15553\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "file_paths = []\n",
    "print('Reading directory: '+str(path))     # r=root directory d=directories f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.pdf' in file:\n",
    "            files.append(file)\n",
    "            file_paths.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Core IDs of Files in directory\n",
    "files_core_id = []\n",
    "for i in range(len(files)):\n",
    "    files_core_id.append(int(re.sub('.pdf','',re.sub('Core_ID_','', str(files[i])))))\n",
    "    \n",
    "len(files_core_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### extract text of 15.553 PDFs (first page)\n",
    "## output the progess in percent every 100 items\n",
    "\n",
    "all_pdf_text = [] \n",
    "lauf=0\n",
    "start_time = datetime.now()\n",
    "for i in range(len(files)):\n",
    "\n",
    "    all_pdf_text.append(extract_page_one(file_paths[i]).getvalue())\n",
    "    if lauf%100==0:\n",
    "        print(str((i/len(files))*100)+'%')\n",
    "    lauf=lauf+1\n",
    "    \n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_delta = (end_time - start_time)\n",
    "seconds_all = time_delta.total_seconds()\n",
    "minutes_all = seconds_all/60\n",
    "print('Time duration for text extractions: '+str(minutes_all)+' min.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all titles from meta data with core_ids of fulltext\n",
    "titles = []\n",
    "for i in range(len(files_core_id)):\n",
    "    index = df_meta.index[df_meta['coreId'] == int(files_core_id[i])].tolist()\n",
    "    if index == []:\n",
    "        titles.append('No metadata found')\n",
    "    else: \n",
    "        index = index[0]\n",
    "        title_pdf  = df_meta.loc[index,'title']\n",
    "        titles.append(title_pdf)\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get author for the PDF´s\n",
    "\n",
    "authors = []\n",
    "for i in range(len(files_core_id)):\n",
    "    index = df_meta.index[df_meta['coreId'] == int(files_core_id[i])].tolist()\n",
    "    index = index[0]\n",
    "    author_pdf  = df_meta.loc[index,'authors']\n",
    "\n",
    "    author_pdf = removeAutor(author_pdf).split(\",\")\n",
    "    for j in range(len(author_pdf)):\n",
    "        author_pdf[j] = ' '.join(author_pdf[j].split()) ## remove excessive whitespaces\n",
    "        \n",
    "    authors.append(author_pdf)\n",
    "len(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that labels the data \n",
    "\n",
    "no_author = []\n",
    "no_title = []\n",
    "error_papers = []\n",
    "result_tokens = []\n",
    "result_label = []\n",
    "count_papers = len(files_core_id)\n",
    "\n",
    "for paper in range(count_papers):\n",
    "    title = ' '.join(removePassage(titles[paper]).split()).lower() # Remove excces Whitespace & to lowercase\n",
    "    title = re.sub(\"\\(\",\"\\(\",title) # () as non-regex string\n",
    "    title = re.sub(\"\\)\",\"\\)\",title)\n",
    "\n",
    "    title_index = re.search(title, ' '.join(all_pdf_text[paper].split()).lower())\n",
    "\n",
    "    \n",
    "    if title_index==None:\n",
    "        no_title.append(files_core_id[paper])\n",
    "    else:\n",
    "        try:\n",
    "            Text_pdf_0 = ' '.join(all_pdf_text[paper].split())\n",
    "            if title_index.start()==0:\n",
    "                teil_B = \"\"\n",
    "            else:\n",
    "                teil_B = Text_pdf_0[0:title_index.start()-1]\n",
    "            teil_T = Text_pdf_0[title_index.start():title_index.end()]\n",
    "            teil_E = Text_pdf_0[title_index.end()+1:len(Text_pdf_0)]\n",
    "\n",
    "            y_teil1 = np.repeat('miscellaneous',len(teil_B.split()))\n",
    "            y_teil2 = np.append(['B-title'],np.repeat('I-title',len(teil_T.split())-1))\n",
    "            y_teil3 = np.repeat('miscellaneous',len(teil_E.split()))\n",
    "\n",
    "            y_final = np.concatenate((y_teil1, y_teil2 , y_teil3), axis=None)\n",
    "\n",
    "            ##### Get Text\n",
    "            all_pdf_text1 = re.sub(\"\\\\n\",\" NEWLINE \",all_pdf_text[paper])\n",
    "            Text_pdf_0_NL = ' '.join(all_pdf_text1.split())\n",
    "\n",
    "            Tokens = Text_pdf_0.split()\n",
    "            Labels = y_final\n",
    "            Real_Tokens = Text_pdf_0_NL.split()\n",
    "\n",
    "            authors_surname = []\n",
    "            for i in range(len(authors[paper])):\n",
    "                if i % 2 == 0:\n",
    "                    authors_surname.append(authors[paper][i])\n",
    "\n",
    "            authors_surname_lower = []\n",
    "            for i in range(len(authors_surname)):\n",
    "                authors_surname_lower.append(authors_surname[i].lower())\n",
    "        \n",
    "            if re.match('.\\.',authors[paper][1]) == None:\n",
    "                authors_forename = []\n",
    "                for i in range(len(authors[paper])):\n",
    "                    if i % 2 == 1:\n",
    "                        authors_forename.append(authors[paper][i].split())\n",
    "\n",
    "                authors_forename = list(np.concatenate((authors_forename), axis=None))\n",
    "                authors_forename_lower = []\n",
    "                for i in range(len(authors_forename)):\n",
    "                    authors_forename_lower.append(authors_forename[i].lower())\n",
    "\n",
    "                authors_surname_lower = list(np.concatenate((authors_forename_lower,authors_surname_lower), axis=None))\n",
    "    \n",
    "            Tokens = all_pdf_text[paper].split()\n",
    "            Tokens_final_lower = []\n",
    "            for i in range(len(Tokens)):\n",
    "                Tokens_final_lower.append(Tokens[i].lower())\n",
    "\n",
    "            vec_author = []\n",
    "            for token in Tokens_final_lower:\n",
    "                line = any(word in token for word in authors_surname_lower)\n",
    "                vec_author.append(line)\n",
    "\n",
    "            index_author = [i for i, e in enumerate(vec_author) if e == True]\n",
    "\n",
    "            if len(index_author)>(len(authors_surname_lower)):\n",
    "                diff = len(index_author) - len(authors_surname_lower)\n",
    "                dist = []\n",
    "                for j in range(len(index_author)):\n",
    "                    dist.append(abs(index_author[j]-y_final_REAL.index('B-title')))\n",
    "\n",
    "                dict1 = dict(zip(dist , index_author))\n",
    "\n",
    "                dist.sort(reverse = True)\n",
    "\n",
    "                for k in range(len(dist[0:diff])):\n",
    "                    vec_author[dict1[dist[0:diff][k]]] = False\n",
    "\n",
    "            for i in range(len(y_final)):\n",
    "                if vec_author[i] == True:\n",
    "                    y_final[i] = 'author'\n",
    "\n",
    "            if True not in vec_author:\n",
    "                no_author.append(files_core_id[paper])\n",
    "                \n",
    "            if re.match('.\\.',authors[paper][1]) != None:\n",
    "\n",
    "                index_author_true = [i for i, e in enumerate(vec_author) if e == True]\n",
    "\n",
    "                for w in range(len(index_author_true)):\n",
    "                    index = index_author_true[w]\n",
    "                    for t in range(index - 4,index + 4):\n",
    "                        if re.match('.\\.',Tokens_final_lower[t]) != None and Tokens[t].isupper():\n",
    "                            y_final[t] = 'author'\n",
    "\n",
    "            RealTokens_final = add_newlines(Tokens,Real_Tokens,y_final)[0]\n",
    "            y_final_REAL = add_newlines(Tokens,Real_Tokens,y_final)[1]\n",
    "\n",
    "            result_label.append(y_final_REAL)\n",
    "            result_tokens.append(RealTokens_final)\n",
    "        except:\n",
    "            error_papers.append(files_core_id[paper])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labled Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = \"D:\\human_readable_xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=0\n",
    "start_time = datetime.now()\n",
    "for i in range(len(files_core_id)):\n",
    "\n",
    "    output_text_token = []\n",
    "    for token, label in zip(result_tokens[i],result_label[i]):\n",
    "        if label != \"miscellaneous\":\n",
    "            output_text_token.append(f\"<{label}>\" + token + f\"</{label}>\")\n",
    "        else:\n",
    "            output_text_token.append(token)\n",
    "\n",
    "    output_text = ' '.join(output_text_token) \n",
    "    myfile = open(path_output + \"\\\\\"+str(files_core_id[i])+\".xml\", \"w\",encoding=\"utf-8\")\n",
    "    myfile.write(output_text)\n",
    "    myfile.close()\n",
    "    if run%100==0:\n",
    "        print(str((i/len(files_core_id))*100)+'%')\n",
    "    run=run+1\n",
    "\n",
    "#     print(output_text)\n",
    "end_time = datetime.now()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
